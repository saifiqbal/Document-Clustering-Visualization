Intelligence without representation*Rodney A. BrooksMIT Artificial Intelligence Laboratory, 545 Technology Square, Rm. 836, Cambridge, MA 02139, USAReceived September 1987Brooks, R.A., Intelligence without representation, Artificial Intelligence 47 (1991), 139320159.* This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for theresearch is provided in part by an IBM Faculty 9 Development Award, in part by a grant from the Systems Development Foundation, in part bythe University Research Initiative under Office of Naval Research contract N00014-86-K-0685 and in part by the Advanced ResearchProjects Agency under Office of Naval Research contract N00014-85-K-0124.AbstractArtificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, withstrict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outlineour approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not intoindependent information processing units which must interface with each other via representations. Instead, the intelligent system isdecomposed into independent and parallel activity producers which all interface directly to the world through perception and action, ratherthan interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central andperipheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures instandard office environments.1. IntroductionArtificial intelligence started as a field whose goalwas to replicate human level intelligence in amachine.Early hopes diminished as the magnitude anddifficulty of that goal was appreciated. Slow progresswas made over the next 25 years in demonstratingisolated aspects of intelligence. Recent work hastended to concentrate on commercializable aspects of"intelligent assistants" for human workers.No one talks about replicating the full gamut ofhuman intelligence any more. Instead we see a retreatinto specialized subproblems, such as ways torepresent knowledge, natural language understanding,vision or even more specialized areas such as truthmaintenance systems or plan verification. All thework in these subareas is benchmarked against thesorts of tasks humans do within those areas.Amongst the dreamers still in the field of AI (thosenot dreaming about dollars, that is), there is a feeling.that one day all these pieces will all fall into placeand we will see "truly" intelligent systems emerge.However, I, and others, believe that human levelintelligence is too complex and little understood to becorrectly decomposed into the right subpieces at themoment and that even if we knew the subpieces westill wouldn't know the right interfaces betweenthem. Furthermore, we will never understand how todecompose human level intelligence until we've had alot of practice with simpler level intelligences.In this paper I therefore argue for a differentapproach to creating artificial intelligence:245 We must incrementally build up the capabilities ofintelligent systems, having complete systems ateach step of the way and thus automatically ensurethat the pieces and their interfaces are valid.245 At each step we should build complete intelligentsystems that we let loose in the real world with realsensing and real action. Anything less provides acandidate with which we can delude ourselves.We have been following this approach and have builta series of autonomous mobile robots. We havereached an unexpected conclusion (C) and have arather radical hypothesis (H).(C)When we examine very simple level intelligencewe find that explicit representations and modelsof the world simply get in the way. It turns outto be better to use the world as its own model.(H)Representation is the wrong unit of abstractionin building the bulkiest parts of intelligentsystems.Representation has been the central issue in artificialintelligence work over the last 15 years only becauseit has provided an interface between otherwise isolatedmodules and conference papers.2. The evolution of intelligenceWe already have an existence proof of, thepossibility of intelligent entities: human beings.Additionally, many animals are intelligent to somedegree. (This is a subject of intense debate, much ofwhich really centers around a definition of intelligence.) They have evolved over the 4.6 billionyear history of the earth.It is instructive to reflect on the way in whichearth-based biological evolution spent its time.Single-cell entities arose out of the primordial souproughly 3.5 billion years ago. A billion years passedbefore photosynthetic plants appeared. After almostanother billion and a half years, around 550 millionyears ago, the first fish and Vertebrates arrived, andthen insects 450 million years ago. Then thingsstarted moving fast. Reptiles arrived 370 millionyears ago, followed by dinosaurs at 330 andmammals at 250 million years ago. The firstprimates appeared 120 million years ago and theimmediate predecessors to the great apes a mere 18million years ago. Man arrived in roughly his presentform 2.5 million years ago. He invented agriculture amere 10,000 years ago, writing less than 5000 yearsago and "expert" knowledge only over the last fewhundred years,This suggests that problem solving behavior,language, expert knowledge and application, andreason, are all pretty simple once the essence of beingand reacting are available. That essence is the abilityto move around in a dynamic environment, sensingthe surroundings to a degree sufficient to achieve thenecessary maintenance of life and reproduction. Thispart of intelligence is where evolution hasconcentrated its time321it is much harder.I believe that mobility, acute vision and the abilityto carry out survivalrelated tasks in a dynamicenvironment provide a necessary basis for thedevelopment of true intelligence. Moravec [11] arguesthis same case rather eloquently.Human level intelligence has provided us with anexistence proof but we must be careful about whatthe lessons are to be gained from it.2. 1. A storySuppose it is the 1890s. Artificial flight is theglamor subject in science, engineering, and venturecapital circles. A bunch of AF researchers  aremiraculously transported by a time machine to the1980s for a few hours. They spend the whole time inthe passenger cabin of a commercial passengerBoeing 747 on a medium duration flight.Returned to the 1890s they feel vigorated, knowingthat AF is possible on a grand scale. Theyimmediately set to work duplicating what they haveseen. They make great progress in designing pitchedseats, double pane windows, and know that if onlythey can figure out those weird "plastics" they willhave their grail within their grasp. (A fewconnectionists amongst them caught a glimpse of anengine with its cover off and they are preoccupiedwith inspirations from that experience.)3. Abstraction as a dangerous weaponArtificial intelligence researchers are fond of pointingout that AI is often denied its rightful successes. Thepopular story goes that when nobody has any goodidea of how to solve a particular sort of problem (e.g.playing chess) it is known as an AI problem. Whenan algorithm developed by AI researchers successfullytackles such a problem, however, AI detractors claimthat since the problem was solvable by an algorithm,it wasn't really an AI problem after all. Thus AInever has any successes. But have you ever heard ofan AI failure?I claim that AI researchers are guilty of the same(self) deception. They partition the problems theywork on into two components. The AI component,which they solve, and the non-AI component which,they don't solve. Typically, AI "succeeds" by definingthe parts of the problem that are unsolved as not AI.The principal mechanism for this partitioning isabstraction. Its application is usually considered partof good science, not, as it is in fact used in AI, as amechanism for self-delusion. In AI, abstraction isusually used to factor out all aspects of perceptionand motor skills. I argue below that these are the hardproblems solved by intelligent systems, and furtherthat the shape of solutions to these problemsconstrains greatly the correct solutions of the smallpieces of intelligence which remain.Early work in AI concentrated   on games,geometrical problems, symbolic algebra, theoremproving, and other formal systems (e.g. [6, 9]). Ineach case the semantics of the domains were fairlysimple.In the late sixties and early seventies the blocksworld became a popular domain for AI research. It hada uniform and simple semantics. The key to successwas to represent the state of the world completely andexplicitly. Search techniques could then be used forplanning within this well-understood world. Learningcould also be done within the blocks world; therewere only a few simple concepts worth learning andthey could be captured by enumerating the set ofsubexpressions which must be contained in anyformal description of a world including an instance ofthe concept. The blocks world was even used forvision research and mobile robotics, as it providedstrong constraints on the perceptual processingnecessary [12].  Merkwelt may not be anything like that used byhumans. In fact, it may be the case that ourintrospective descriptions of our internalrepresentations are completely misleading and quitedifferent from what we really use.3.1. A continuing storyMeanwhile our friends in the 1890s are busy atwork on their AF machine. They have come to agreethat the project is too big to be worked on as a singleentity and that they will need to become specialists indifferent areas. After all, they had asked questions offellow passengers on their flight and discovered thatthe Boeing Co. employed over 6000 people to buildsuch an airplane. Everyone is busy but there is not a lot ofcommunication between the groups. The peoplemaking the passenger seats used the finest solid steelavailable as the framework. There was somemuttering that perhaps they should use tubular steelto save weight, but the general consensus was that ifsuch an obviously big and heavy airplane could flythen clearly there was no problem with weight.On their observation flight none of the originalgroup managed to get a glimpse of the driver's seat,but they have done some hard thinking and think theyhave established the major constraints on what shouldbe there and how it should work. The pilot, as hewill be called, sits in a seat above a glass floor sothat he can see the ground below so he will knowwhere to land. There are some side mirrors so he canwatch behind for other approaching airplanes. Hiscontrols consist of a foot pedal to control speed (justas in these newfangled automobiles that are startingto appear), and a steering wheel to turn left and right.In addition, the wheel stem can be pushed forward andback to make the airplane go up and down. A cleverarrangement of pipes measures airspeed of theairplane and displays it on a dial. What more couldone want? Oh yes. There's a rather nice setup oflouvers in the windows so that the driver can getfresh air without getting the full blast of the wind inhis face.An interesting sidelight is that all the researchershave by now abandoned the study of aerodynamics.Some of them had intensely questioned their fellowpassengers on this subject and not one of the modernflyers had known a thing about it. Clearly the AFresearchers had previously been wasting their time inits pursuit.4. Incremental intelligenceI wish to build completely autonomous mobileagents that co-exist in the world with humans, andare seen by those humans as intelligent beings intheir own right. I will call such agents Creatures.This is my intellectual motivation. I have noparticular interest in demonstrating how humanbeings work, although humans, like other animals,are interesting objects of study in this endeavor asthey are successful autonomous agents. I have noparticular interest in applications  it seems clear tome that if my goals can be met then the range ofapplications for such Creatures will be limited onlyby our (or their) imagination. I have no particularinterest in the philosophical implications ofCreatures, although clearly there will be significantimplications.Given the caveats of the previous two sections andconsidering the parable of the AF researchers, I amconvinced that I must tread carefully in this endeavorto avoid some nasty pitfalls.For the moment then, consider the problem ofbuilding Creatures as an engineering problem. Wewill develop an engineering methodology for buildingCreatures.First, let us consider some of the requirements for ourCreatures.245 A Creature must cope appropriately and in a timelyfashion with changes in its dynamic environment.245 A Creature should be robust with respect to itsenvironment; minor changes in the properties ofthe world should not lead to total collapse of theCreature's behavior; rather one should expect only agradual change in capabilities of the Creature as theenvironment changes more and more.245 A Creature should be able to maintain multiplegoals and, depending on the circumstances it findsitself in, change which particular goals it isactively pursuing; thus it can both adapt tosurroundings and capitalize on fortuitouscircumstances.245 A Creature should do something in the world; itshould have some purpose in being.Now, let us consider some of the valid engineeringapproaches to achieving these requirements. As in allengineering endeavors it is necessary to decompose acomplex system into parts, build the parts, theninterface them into a complete system.4. 1. Decomposition by function.   looking at the state of both the first and secondprocesses that that number can be given anyinterpretation at all. An extremist might say that wereally do have representations, but that they are justimplicit. With an appropriate mapping of thecomplete system and its state to another domain, wecould define a representation that these numbers andtopological connections between processes somehowencode.However we are not happy with calling suchthings a representation. They differ from standardrepresentations in too many ways.There are no variables (e.g. see [1] for a morethorough treatment of this) that need instantiation inreasoning processes. There are no rules which need tobe selected through pattern matching. There are nochoices to be made. To a large extent the state of theworld determines the action of the Creature. Simon[14] noted that the complexity of behavior of asystem was not necessarily inherent in thecomplexity of the creature, but Perhaps in thecomplexity of the environment. He made thisanalysis in his description of an Ant wandering thebeach, but ignored its implications in the nextparagraph when he talked about humans. Wehypothesize (following Agre and Chapman) thatmuch of even human level activity is similarly areflection of the world through very simplemechanisms without detailed representations.6. The methodology, in practiceIn order to build systems based on an activitydecomposition so that they are truly robust we mustrigorously follow a careful methodology.6. 1. Methodological maximsFirst, it is vitally important to test the Creatureswe build in the real world; i.e., in the same worldthat we humans inhabit. It is disastrous to fall intothe temptation of testing them in a simplified worldfirst, even with the best intentions of latertransferring activity to an unsimplified world. With asimplified world (matte painted walls, rectangularvertices everywhere, colored blocks as the onlyobstacles) it is very easy to accidentally build asubmodule of the system which happens to rely onsome of those simplified properties. This reliance canthen easily be reflected in the requirements on theinterfaces between that submodule and others. Thedisease spreads and the complete system depends in asubtle way on the simplified world. When it comestime to move to the, unsimplified world, wegradually and painfully realize that every piece of thesystem must be rebuilt. Worse than that we may needto rethink the total design as the issues may changecompletely. We are not so concerned that it might bedangerous to test simplified Creatures first and lateradd more sophisticated layers of control becauseevolution has been successful using this approach.Second, as each layer is built it must be testedextensively in the real world. The system mustinteract with the real world over extended periods. Itsbehavior must be observed and be carefully andthoroughly debugged. When a second layer is added toan existing layer there are three potential sources ofbugs: the first layer, the second layer, or theinteraction of the two layers. Eliminating the first ofthese source of bugs as a possibility makes findingbugs much easier. Furthermore, there is only onething possible to vary in order to fix the bugs321thesecond layer.6.2. An instantiation of the methodologyWe have built a series of four robots based on themethodology of task decomposition. They all operatein an unconstrained dynamic world (laboratory andoffice areas in the MIT Artificial IntelligenceLaboratory). They successfully operate with peoplewalking by, people deliberately trying to confusethem, and people just standing by watching them.All four robots are Creatures in the sense that onpower-up they exist in the world and interact with it,pursuing multiple goals determined by their controllayers implementing different activities. This is incontrast to other mobile robots that are givenprograms or plans to follow for a specific mission,The four robots are shown in Fig. 1. Two areidentical, so there are really three, designs. One usesan offboard LISP machine for most of itscomputations, two use onboard combinationalnetworks, and one uses a custom onboard parallelprocessor. All the robots implement the sameabstract architecture, which we call the subsumptionarchitecture which embodies the fundamental ideasof decomposition into layers of task achievingbehaviors, and incremental composition throughdebugging in the real world. Details of theseimplementations can be found in [3].Each layer in the subsumption architecture iscomposed of a fixed-topology network of simplefinite state machines. Each finite state machine has ahandful of states, one or two internal registers, one ortwo internal timers, and access to simplecomputational machines, which can compute thingssuch as vector sums. The finite state machines runasynchronously, sending and receiving fixed lengthmessages (1-bit messages on the two small robots, and 24-bit messages on the larger ones) over wires.On our first robot these were virtual wires; on ourlater robots we have used physical wires to connectcomputational components.There is no central locus of control. Rather, the finitestate machines are data-driven by the messages theyreceive. The arrival of messages or the expiration ofdesignated time periods cause the finite statemachines to change state. The finite state machineshave access to the contents of the messages andmight output them, test them with a predicate andconditionally branch to a different state, or pass themto simple computation elements. There is nopossibility of access to global data, nor ofdynamically established communications links. Thereis thus no possibility of global control. All finitestate machines are equal, yet at the same time theyare prisoners of their fixed topology connections.Layers are combined through mechanisms we callsuppression (whence the name subsumptionarchitecture) and inhibition. In both cases as a newlayer is added, one of the new wires is side-tappedinto an existing wire. A pre-defined time constant isassociated with each side-tap. In the case ofsuppression the side-tapping occurs on the input sideof a finite state machine. If a message arrives on thenet wire it is directed to the input port of the finitestate machine as though it had arrived on the existingwire. Additionally, any new messages on the existingwire are suppressed (i.e., rejected) for the specifiedtime period. For inhibition the side-tapping occurs onthe output side of a finite state machine. A messageon the new wire simply inhibits messages beingemitted on the existing wire for the specified timeperiod. Unlike suppression the new message is notdelivered in their place.As an example, consider the three layers of Fig. 2.These are three layers of control that we have run onour first mobile robot for well over a year. The robothas a ring of twelve ultrasonic sonars as its primarysensors. Every second these sonars are run to givetwelve radial depth measurements. Sonar is extremelynoisy due to many objects being mirrors to sonar.There are thus problems with specular reflection andreturn paths following multiple reflections due tosurface skimming with low angles of incidence (lessthan thirty degrees).In more detail the three layers work as follows: Fig. 1. The four MIT AI laboratory Mobots. Left-most is the firstbuilt Allen, which relies on an offboard LISP machine forcomputation support. The right-most one is Herbert, shown with a24 node CMOS parallel processor surrounding its girth. Newsensors and fast early vision processors are still to be built andinstalled. In the middle are Tom and Jerry, based on acommercial toy chassis, with single PALs (Programmable Arrayof Logic) as their controllers. (1) The lowest-level layer implements a behaviorwhich makes the robot (the physical embodiment ofthe Creature) avoid hitting objects. It both avoidsstatic objects and moving objects, even those that areactively attacking it. The finite state machine labelledsonar simply runs the sonar devices and every secondemits an instantaneous map with the readingsconverted to polar coordinates. This map is passed onto the collide and feelforce finite state machine. Thefirst of these simply watches to see if there isanything dead ahead, and if so sends a halt message tothe finite state machine in charge of running therobot forwards321if that finite state machine is not inthe correct state the message may well be ignored.Simultaneously, the other finite state machinecomputes a repulsive force on the robot, based on aninverse square law, where each sonar return isconsidered to indicate the presence of a repulsiveobject. The contributions from each sonar are added toproduce an overall force acting on the robot. Theoutput is passed to the runaway machine whichthresholds it and passes it on to the turn machinewhich orients the robot directly away from thesummed repulsive force. Finally, the forwardmachine drives the robot forward. Whenever thismachine receives a halt message while the robot isdriving forward, it commands the robot to halt.This network of finite state machines generatesbehaviors which let the robot avoid objects. If itstarts in the middle of an empty room it simply sitsthere. If someone walks up to it, the robot movesaway. If it moves in the direction of other obstacles ithalts. Overall, it manages to exist in a dynamicenvironment without hitting or being hit by objects. The next layer makes the robot wander about,when not busy avoiding objects. The wander finitestate machine generates a random heading for therobot every ten seconds or so. The avoid machinetreats that heading as an attractive force and sums itwith the repulsive force computed from the sonars. Ituses the result to suppress the lower-level behavior,forcing the robot to move in a direction close to whatwander decided but at the same time avoid anyobstacles. Note that if the. turn and forward finitestate machines are busy running the robot the newimpulse to wander will be ignored.(3) The third layer makes the robot try to explore.It looks for distant places, then tries to reach them.This layer suppresses the wander layer, and observeshow the bottom layer diverts the robot due. toobstacles, (perhaps dynamic). It corrects for anydivergences and the robot achieves the goal. Fig. 2. We wire, finite state machines together into layers ofcontrol. Each layer is built on top of existing layers. Lower levellayers never rely on the existence of higher level layers.The whenlook finite state machine notices whenthe robot is not busy moving, and starts up, the freespace finder (labelled stereo in the diagram) finitestate machine. At the same time it inhibits wanderingbehavior so that the observation will remain valid.When a path is observed it is sent to the pathplanfinite state machine, which injects a commandeddirection to the avoid finite state machine. In thisway, lower-level obstacle avoidance continues tofunction. This may cause the robot to go in adirection different to that desired by pathplan. Forthat reason the actual path of the robot is monitoredby the integrate finite state machine, which sendsupdated estimates to the pathplan machine. Thismachine then acts as a difference engine forcing therobot in the desired direction and compensating forthe actual path of the robot as it avoids obstacles.These particular layers were implemented on ourfirst robot. See [3] for more details. Brooks andConnell [5] report on another three layersimplemented on that particular robot.7. What this is notThe subsumption architecture with its network ofsimple machines is reminiscent, at the surface levelat least, with a number of mechanistic approaches tointelligence, such as connectionism and neuralnetworks. But it is different in many respects forthese endeavors, and also quite different from manyother post-Dartmouth traditions in artificialintelligence. We very briefly explain those differencesin the following sections.7.1. It isn't connectionismConnectionists try to make networks of simpleprocessors. In that regard, the things they build (insimulation only321no connectionist has ever driven areal robot in a real environment, no matter howsimple) are similar to the subsumption networks webuild. However, their processing nodes tend to beuniform and they are looking (as their name suggests)for revelations from understanding how to connectthem correctly (which is usually assumed to meanrichly at least). Our nodes are all unique finite statemachines and the density of connections is very muchlower, certainly not uniform, and very low indeedbetween layers. Additionally, connectionists seem tobe looking for explicit distributed representations tospontaneously arise from their networks. We harborno such hopes because we believe representations arenot necessary and appear only in the eye or mind ofthe observer.7.2. It isn't neural networksNeural networks is the parent discipline of whichconnectionism is a recent incarnation. Workers inneural networks claim that there is some biologicalsignificance to their network nodes, as models ofneurons. Most of the, models seem wildlyimplausible given the paucity of modeled connectionsrelative to the thousands found in real neurons. Weclaim no biological significance in our choice offinite state machines as network nodes.7.3. It isn't production rulesEach individual activity producing layer of ourarchitecture could be viewed as an implementation ofa production rule. When the right conditions are metin the environment a certain action will be performed. We feel that analogy is a little like saying that anyFORTRAN program with IF statements isimplementing a production rule system. A standardproduction system really is more321it has a rule base,from which a rule is selected based on matchingpreconditions of all the rules to some database. Thepreconditions may include variables which must bematched to individuals in the database, but layers runin parallel and have no variables or need formatching. Instead, aspects of the world are extractedand these directly trigger or modify certain behaviorsof the layer.7.4. It isn't a blackboardIf one, really wanted, one could make an analogyof our networks to a blackboard, control architecture.Some of the finite state machines would be localizedknowledge sources. Others would be processes actingon these knowledge sources by finding them on theblackboard. There is a simplifying point in our,architecture however: all the processes know exactlywhere to  look on the blackboard as they arehard-wired to the correct place. I think this forcedanalogy indicates its own weakness. There is noflexibility at all on where a process can gatherappropriate knowledge. Most advanced blackboardarchitectures make heavy use of the general sharingand availability of almost all knowledge.Furthermore, in spirit at least, blackboard systemstend to hide from a consumer of knowledge who theparticular producer  was. This is the primary meansof abstraction in blackboard systems. In our systemwe make such connections explicit and permanent.7.5. It isn't German philosophyIn some circles much credence is given toHeidegger as one who understood the dynamics ofexistence. Our approach has certain similarities towork inspired by this German philosopher (e.g. [1])but our work was not so inspired. It is based purelyon engineering considerations. That does not precludeit from being used in philosophical debate as anexample on any side of any fence, however.8. Limits to growthSince our approach is a performance-based one, itis the performance of the systems we build whichmust be used to measure its usefulness and to pointto its limitations.We claim that as of mid-1987 our robots, usingthe subsumption architecture to implement completeCreatures, are the most reactive real-time mobilerobots in existence. Most other mobile robots arestill at the stage of individual "experimental runs" instatic environments, or at best in completely mappedstatic environments. Ours, on the other hand, operatecompletely autonomously in complex dynamicenvironments at the flick of their on switches, andcontinue until their batteries are drained. We believethey operate at a level closer to simple insect levelintelligence than to bacteria level intelligence. Ourgoal (worth nothing if we don't deliver) is simpleinsect level intelligence within two years. Evolutiontook 3 billion years to get from single cells toinsects, and only another 500 million years fromthere to humans. This statement is not intended as aprediction of our future performance, but rather toindicate the nontrivial nature of insect levelintelligence.Despite this good performance to date, there are anumber of serious questions about our approach. Wehave beliefs and hopes about how these questionswill be resolved, but under our criteria onlyperformance truly counts. Experiments and buildingmore complex systems take time, so with the caveatthat the experiments described below have not yetbeen performed we outline how we currently see ourendeavor progressing. Our intent in discussing this isto indicate that there is at least a plausible pathforward to more intelligent machines from our currentsituation.Our belief is that the sorts of activity producinglayers of control we are developing (mobility, visionand survival related tasks) are necessary prerequisitesfor higher-level intelligence in the style we attributeto human beings.The most natural and serious questions concerninglimits of our approach are:245 How many layers can be built in the subsumptionarchitecture before the interactions between layersbecome too complex to continue?245 How complex can the behaviors be that aredeveloped without the aid of central representations?245 Can higher-level functions such as learning occur inthese fixed topology networks of simple finite statemachines?We outline our current thoughts on these questions.8.1. How many layers?The highest number of layers we have run on aphysical robot is three. In simulation we have run sixparallel layers. The technique of completelydebugging the robot on all existing activity  [2]R.J. Bobrow and J.S. Brown, Systematic understanding:synthesis, analysis, and contingent knowledge in specializedunderstanding systems, in: R.J. Bobrow and A.M, Collins, eds.,Representation and Understanding (Academic Press, NewYork, 1975) 103-129.[3]R.A. Brooks, A robust layered control system for a mobilerobot, IEEE J. Rob. Autom. 2 (1986) 14-23.[41R.A. Brooks, A hardware retargetable distributed layeredarchitecture for mobile robot control, in: Proceedings IEEERobotics and Automation, Raleigh, NC (1987) 106-110. [5]R.A. Brooks and J.H. Connell, Asynchronous distributedcontrol system for a mobile robot, in: Proceedings SPIE,Cambridge, MA (1986) 77-84.[6]E.A. Feigenbaum and J.A. Feldman, eds., Computers andThought (McGraw-Hill, San Francisco, CA, 1963).[7]J.L. Gould and P. Marler, Learning by instinct, Sci. Am.(1986) 74-85.[8]A.C. Lewis, Memory constraints and Rower choice in pierisrapae, Science 232 (1986) 863-865.[9] M.L. Minsky, ed., Semantic Information Processing (MITPress, Cambridge, MA, 1968).[10] M.L. Minsky, Society of Mind (Simon and Schuster, NewYork, 1986).[11] H.P. Moravec, Locomotion, vision and intelligence, in: M.Brady and R. Paul, eds., Robotics Research 1 (MIT Press,Cambridge, MA, (1984) 215-224.[12] N.J. Nilsson, Shakey the robot, Tech. Note 323, SRI AICenter, Menlo Park, CA (1984).[13] E.H. Shortliffe, MYCIN: Computer-Based MedicalConsultations (Elsevier, New York, 1976).[14] H.A. Simon, The Sciences of the Artificial (MIT Press,Cambridge, MA, 1969).[15] J. Von Uexk237ll, Umwelt and Innenwelt der Tiere (Berlin,1921). 